{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a5fa65-95ca-4a24-a338-bca1642af01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['income_bracket', 'employment_status', 'education_level', 'risk_appetite_label', 'holdings']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'X_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 232\u001b[0m     model, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_risk_appetite_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclient_portfolio_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete with test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m, in \u001b[0;36mtrain_risk_appetite_model\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(f\"Numerical features: {numerical_features}\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# # Extract target variable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 4. Feature Preprocessing\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create preprocessing for numerical and categorical features\u001b[39;00m\n\u001b[1;32m     57\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     58\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     59\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(), numerical_features),\n\u001b[1;32m     60\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m), categorical_features)\n\u001b[1;32m     61\u001b[0m     ])\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 5. Model Selection & Training\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'X_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_risk_appetite_model(data_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # 1. Data Exploration & Preprocessing\n",
    "    # print(f\"Dataset shape: {df.shape}\")\n",
    "    # print(df.info())\n",
    "    # print(df.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    # print(f\"Missing values:\\n{missing_values}\")\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    df = df.fillna(df.median(numeric_only=True))\n",
    "    \n",
    "    # 2. Feature Engineering\n",
    "    df = df.drop(['client_id'], axis=1)\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    # print(f\"Numerical features: {numerical_features}\")\n",
    "\n",
    "    # # Extract target variable\n",
    "    # X = df.drop([ 'risk_appetite_label'], axis=1)\n",
    "    # y = df['risk_appetite_label']\n",
    "    \n",
    "    # # 3. Data Splitting with 70:15:15 ratio\n",
    "    # # First split: 70% train, 30% temp\n",
    "    # X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # # Second split: Split the temp into validation and test (50% each, which is 15% of original)\n",
    "    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    # print(f\"Train set: {X_train.shape[0]} samples ({100*X_train.shape[0]/X.shape[0]:.1f}%)\")\n",
    "    # print(f\"Validation set: {X_val.shape[0]} samples ({100*X_val.shape[0]/X.shape[0]:.1f}%)\")\n",
    "    # print(f\"Test set: {X_test.shape[0]} samples ({100*X_test.shape[0]/X.shape[0]:.1f}%)\")\n",
    "\n",
    "    # 4. Feature Preprocessing\n",
    "    # Create preprocessing for numerical and categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    # 5. Model Selection & Training\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42),\n",
    "        'LightGBM': LGBMClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        # Create pipeline with preprocessing and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Extract target variable\n",
    "        X = df.drop([ 'risk_appetite_label'], axis=1)\n",
    "        y = df['risk_appetite_label']\n",
    "        \n",
    "        # 3. Data Splitting with 70:15:15 ratio\n",
    "        # First split: 70% train, 30% temp\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        \n",
    "        # Second split: Split the temp into validation and test (50% each, which is 15% of original)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate model\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"{name} Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = pipeline\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nBest model is {best_model_name} with validation accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # 6. Hyperparameter Tuning for the best model\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__max_depth': [None, 10, 20, 30],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'model__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'model__max_depth': [3, 5, 7],\n",
    "            'model__subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    else:  # LightGBM\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'model__max_depth': [3, 5, 7],\n",
    "            'model__num_leaves': [31, 63, 127]\n",
    "        }\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 7. Feature Importance Analysis\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on validation set again\n",
    "    y_val_pred = final_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"\\nFinal model validation accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # 8. Feature Selection based on importance\n",
    "    # Get feature importances if model supports it\n",
    "    if hasattr(final_model.named_steps['model'], 'feature_importances_'):\n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        for name, transformer, columns in preprocessor.transformers_:\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                if name == 'cat':\n",
    "                    feature_names.extend(transformer.get_feature_names_out(columns))\n",
    "                else:\n",
    "                    feature_names.extend(columns)\n",
    "            else:\n",
    "                feature_names.extend(columns)\n",
    "        \n",
    "        importances = final_model.named_steps['model'].feature_importances_\n",
    "        if len(feature_names) == len(importances):\n",
    "            feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nTop 10 important features:\")\n",
    "            print(feature_importance.head(10))\n",
    "            \n",
    "            # Plot feature importances\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "            plt.title('Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance.png')\n",
    "            \n",
    "            # Use top features for a simpler model\n",
    "            sfm = SelectFromModel(final_model.named_steps['model'], threshold='median')\n",
    "            sfm.fit(preprocessor.transform(X_train), y_train)\n",
    "            \n",
    "            # Create a pipeline with selected features\n",
    "            selected_features_model = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('feature_selection', sfm),\n",
    "                ('model', final_model.named_steps['model'])\n",
    "            ])\n",
    "            \n",
    "            selected_features_model.fit(X_train, y_train)\n",
    "            y_val_pred_selected = selected_features_model.predict(X_val)\n",
    "            selected_accuracy = accuracy_score(y_val, y_val_pred_selected)\n",
    "            \n",
    "            print(f\"\\nModel with selected features validation accuracy: {selected_accuracy:.4f}\")\n",
    "            \n",
    "            if selected_accuracy >= val_accuracy:\n",
    "                final_model = selected_features_model\n",
    "                print(\"Using model with selected features as it performs better or equally well.\")\n",
    "    \n",
    "    # 9. Final Evaluation on Test Set\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"\\nFinal model test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Print classification report for detailed metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    \n",
    "    # 10. Save the final model\n",
    "    import joblib\n",
    "    joblib.dump(final_model, 'risk_appetite_model.pkl')\n",
    "    print(\"\\nModel saved as 'risk_appetite_model.pkl'\")\n",
    "    \n",
    "    return final_model, test_accuracy\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    model, accuracy = train_risk_appetite_model('client_portfolio_data.csv')\n",
    "    print(f\"Model training complete with test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff4aa7-b7c1-46af-b969-b7191206ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_risk_appetite_model(client_portfolio_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c37598-5f9b-4ff1-aa09-cbfac3ae29d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-3.0.3-py3-none-manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bef155-d253-474a-b41d-0c60ba2a6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.15.2)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.6.0-py3-none-linux_x86_64.whl size=2737778 sha256=548676bd080a312841ff78db3a2ed24424ffb9ee5352f4cf691dd1376422bbcf\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bb/db/6d/7814aed03437129dc284a055c084f201b765deb54b6908efab\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1797e238-ca78-4a37-83d8-29cf3259931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement XGBClassifier (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for XGBClassifier\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bf25f-e6db-4dc1-ba4f-25a226b7d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('client_portfolio_data.csv')\n",
    "\n",
    "# Examine basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df['risk_appetite_label'].value_counts())\n",
    "\n",
    "# Data preprocessing\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['client_id', 'income_bracket', 'employment_status', 'education_level', 'holdings']\n",
    "numerical_cols = [col for col in df.columns if col not in categorical_cols + ['risk_appetite_label']]\n",
    "\n",
    "# Handle missing values\n",
    "for col in numerical_cols:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "for col in categorical_cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop client_id as it's an identifier\n",
    "if 'client_id' in df.columns:\n",
    "    categorical_cols.remove('client_id')\n",
    "    df = df.drop('client_id', axis=1)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('risk_appetite_label', axis=1)\n",
    "y = df['risk_appetite_label']\n",
    "\n",
    "# Split the data into training, validation and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearch with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluation on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Confusion Matrix for validation set\n",
    "plt.figure(figsize=(10, 8))\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Validation Set)')\n",
    "plt.savefig('confusion_matrix_validation.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluation on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix for test set\n",
    "plt.figure(figsize=(10, 8))\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.savefig('confusion_matrix_test.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature importance (for the Random Forest component)\n",
    "if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == 'cat':\n",
    "            # Get one-hot encoded feature names for categorical variables\n",
    "            for i, col in enumerate(cols):\n",
    "                categories = trans.categories_[i]\n",
    "                for cat in categories:\n",
    "                    feature_names.append(f\"{col}_{cat}\")\n",
    "        else:\n",
    "            # Add numerical feature names as is\n",
    "            feature_names.extend(cols)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importances for Risk Appetite Prediction')\n",
    "    plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importances.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print top 15 features\n",
    "    print(\"\\nTop 15 important features:\")\n",
    "    for i in indices[:15]:\n",
    "        print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'risk_appetite_model.pkl')\n",
    "print(\"\\nModel saved as 'risk_appetite_model.pkl'\")\n",
    "\n",
    "# Function for predicting risk appetite for new clients\n",
    "def predict_risk_appetite(client_data):\n",
    "    \"\"\"\n",
    "    Predict risk appetite for a new client\n",
    "    \n",
    "    Parameters:\n",
    "    client_data (dict): Dictionary containing client features\n",
    "    \n",
    "    Returns:\n",
    "    str: Predicted risk appetite label\n",
    "    \"\"\"\n",
    "    # Convert dictionary to DataFrame\n",
    "    client_df = pd.DataFrame([client_data])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(client_df)[0]\n",
    "    \n",
    "    # Get probability scores\n",
    "    probabilities = best_model.predict_proba(client_df)[0]\n",
    "    prob_dict = {best_model.classes_[i]: prob for i, prob in enumerate(probabilities)}\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "print(\"\\nExample of using the prediction function:\")\n",
    "print(\"predict_risk_appetite({'age': 35, 'income_bracket': 'High', ...})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
