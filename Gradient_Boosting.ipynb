{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aab90dc-af25-4a35-be38-b915ced569d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2000, 29)\n",
      "risk_appetite_label\n",
      "Moderate                   622\n",
      "Moderately Conservative    521\n",
      "Conservative               385\n",
      "Moderately Aggressive      298\n",
      "Aggressive                 174\n",
      "Name: count, dtype: int64\n",
      "Training set size: 1400\n",
      "Validation set size: 300\n",
      "Test set size: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27052/3420374621.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_27052/3420374621.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Performance:\n",
      "Accuracy: 0.9667\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             Aggressive       0.89      0.92      0.91        26\n",
      "           Conservative       0.98      0.98      0.98        58\n",
      "               Moderate       0.97      1.00      0.98        93\n",
      "  Moderately Aggressive       0.95      0.87      0.91        45\n",
      "Moderately Conservative       0.99      0.99      0.99        78\n",
      "\n",
      "               accuracy                           0.97       300\n",
      "              macro avg       0.96      0.95      0.95       300\n",
      "           weighted avg       0.97      0.97      0.97       300\n",
      "\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy: 0.9433\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             Aggressive       0.92      0.88      0.90        26\n",
      "           Conservative       0.98      0.95      0.96        58\n",
      "               Moderate       0.93      0.99      0.96        94\n",
      "  Moderately Aggressive       0.90      0.82      0.86        44\n",
      "Moderately Conservative       0.96      0.97      0.97        78\n",
      "\n",
      "               accuracy                           0.94       300\n",
      "              macro avg       0.94      0.92      0.93       300\n",
      "           weighted avg       0.94      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('client_portfolio_data.csv')\n",
    "\n",
    "# Examine basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df['risk_appetite_label'].value_counts())\n",
    "\n",
    "# Data preprocessing\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['client_id', 'income_bracket', 'employment_status', 'education_level', 'holdings']\n",
    "numerical_cols = [col for col in df.columns if col not in categorical_cols + ['risk_appetite_label']]\n",
    "\n",
    "# Handle missing values\n",
    "for col in numerical_cols:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "for col in categorical_cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop client_id as it's an identifier\n",
    "if 'client_id' in df.columns:\n",
    "    categorical_cols.remove('client_id')\n",
    "    df = df.drop('client_id', axis=1)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('risk_appetite_label', axis=1)\n",
    "y = df['risk_appetite_label']\n",
    "\n",
    "# Split the data into training, validation and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "\n",
    "# GridSearch with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluation on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Confusion Matrix for validation set\n",
    "plt.figure(figsize=(10, 8))\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Validation Set)')\n",
    "plt.savefig('confusion_matrix_validation.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluation on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78412d5-9a6b-485b-8a15-5f9be05e9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'Gradient_Boosting_risk_appetite_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for test set\n",
    "plt.figure(figsize=(10, 8))\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Test Set)')\n",
    "plt.savefig('confusion_matrix_test.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature importance (for the Random Forest component)\n",
    "if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    for name, trans, cols in preprocessor.transformers:\n",
    "        if name == 'cat':\n",
    "            # Get one-hot encoded feature names for categorical variables\n",
    "            for i, col in enumerate(cols):\n",
    "                categories = trans.categories[i]\n",
    "                for cat in categories:\n",
    "                    feature_names.append(f\"{col}_{cat}\")\n",
    "        else:\n",
    "            # Add numerical feature names as is\n",
    "            feature_names.extend(cols)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importances for Risk Appetite Prediction')\n",
    "    plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "    # plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importances.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print top 15 features\n",
    "    # print(\"\\nTop 15 important features:\")\n",
    "    # for i in indices[:15]:\n",
    "    #     print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'Gradient_Boosting_risk_appetite_model.pkl')\n",
    "print(\"\\nModel saved as 'Gradient_Boosting_risk_appetite_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef339d91-330a-46f5-8477-d341155b9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of using the prediction function:\n",
      "Prediction\n",
      "['Conservative']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample of using the prediction function:\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "with open('Gradient_Boosting_risk_appetite_model.pkl', 'rb') as file:\n",
    "    loaded_model =  joblib.load(file)\n",
    "df = pd.read_csv('client_portfolio_data.csv')\n",
    "last = df.drop('risk_appetite_label', axis=1).tail(1)\n",
    "predictions = loaded_model.predict(last)\n",
    "print(\"Prediction\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
